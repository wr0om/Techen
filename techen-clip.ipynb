{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9140109,"sourceType":"datasetVersion","datasetId":5520144}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git","metadata":{"execution":{"iopub.status.busy":"2024-08-09T22:36:34.330171Z","iopub.execute_input":"2024-08-09T22:36:34.330856Z","iopub.status.idle":"2024-08-09T22:36:52.369456Z","shell.execute_reply.started":"2024-08-09T22:36:34.330822Z","shell.execute_reply":"2024-08-09T22:36:52.368465Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-uqb54i_d\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-uqb54i_d\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (21.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.16.2)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->clip==1.0) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2024.5.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nDownloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=1dd4490d9e29773eb77eb9cbdc39b10b908d465df1b35e3040cfafc95b73b86c\n  Stored in directory: /tmp/pip-ephem-wheel-cache-el01q93d/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built clip\nInstalling collected packages: ftfy, clip\nSuccessfully installed clip-1.0 ftfy-6.2.3\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport clip\nfrom PIL import Image\nimport requests\nimport pandas as pd\nimport ast\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-09T22:36:52.371437Z","iopub.execute_input":"2024-08-09T22:36:52.371750Z","iopub.status.idle":"2024-08-09T22:36:57.908200Z","shell.execute_reply.started":"2024-08-09T22:36:52.371721Z","shell.execute_reply":"2024-08-09T22:36:57.907273Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"united_divided_df = pd.read_csv('/kaggle/input/united-divided-dataset/united_divided.csv')\nunited_divided_df['logits'] = united_divided_df['logits'].apply(ast.literal_eval)\nunited_divided_df.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T22:36:57.909381Z","iopub.execute_input":"2024-08-09T22:36:57.909810Z","iopub.status.idle":"2024-08-09T22:37:01.864474Z","shell.execute_reply.started":"2024-08-09T22:36:57.909784Z","shell.execute_reply":"2024-08-09T22:37:01.863392Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                          image_link  temperature  \\\n0  https://cdn.pixabay.com/photo/2020/10/03/11/08...          0.8   \n\n                                         description  \\\n0   The image features a woman holding out her ha...   \n\n                                              logits  \\\n0  [( The, {'The': 0.6002, 'In': 0.3314, 'A': 0.0...   \n\n                                      hallucinations  \\\n0  The image features a woman holding out her han...   \n\n                                              hedges            probe_1  \\\n0  The image features a woman holding out her han...  There is handbag.   \n\n   label_1    pred_1                                          context_1  ...  \\\n0    False  0.527317   The image features a woman holding out her ha...  ...   \n\n             probe_3  label_3    pred_3  \\\n0  The dog is white.    False  0.507812   \n\n                                           context_3  \\\n0   The image features a woman holding out her ha...   \n\n                          probe_4  label_4    pred_4  \\\n0  The dog is far from the woman.    False  0.766294   \n\n                                           context_4 group_num  split  \n0   The image features a woman holding out her ha...         5  train  \n\n[1 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_link</th>\n      <th>temperature</th>\n      <th>description</th>\n      <th>logits</th>\n      <th>hallucinations</th>\n      <th>hedges</th>\n      <th>probe_1</th>\n      <th>label_1</th>\n      <th>pred_1</th>\n      <th>context_1</th>\n      <th>...</th>\n      <th>probe_3</th>\n      <th>label_3</th>\n      <th>pred_3</th>\n      <th>context_3</th>\n      <th>probe_4</th>\n      <th>label_4</th>\n      <th>pred_4</th>\n      <th>context_4</th>\n      <th>group_num</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://cdn.pixabay.com/photo/2020/10/03/11/08...</td>\n      <td>0.8</td>\n      <td>The image features a woman holding out her ha...</td>\n      <td>[( The, {'The': 0.6002, 'In': 0.3314, 'A': 0.0...</td>\n      <td>The image features a woman holding out her han...</td>\n      <td>The image features a woman holding out her han...</td>\n      <td>There is handbag.</td>\n      <td>False</td>\n      <td>0.527317</td>\n      <td>The image features a woman holding out her ha...</td>\n      <td>...</td>\n      <td>The dog is white.</td>\n      <td>False</td>\n      <td>0.507812</td>\n      <td>The image features a woman holding out her ha...</td>\n      <td>The dog is far from the woman.</td>\n      <td>False</td>\n      <td>0.766294</td>\n      <td>The image features a woman holding out her ha...</td>\n      <td>5</td>\n      <td>train</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 24 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"probe_cols = [f'probe_{i}' for i in range(1,5)]\nlabel_cols = [f'label_{i}' for i in range(1,5)]\nlink_desc_df = united_divided_df[['image_link', 'split',*probe_cols, *label_cols]]","metadata":{"execution":{"iopub.status.busy":"2024-08-09T22:37:01.867380Z","iopub.execute_input":"2024-08-09T22:37:01.867835Z","iopub.status.idle":"2024-08-09T22:37:01.873996Z","shell.execute_reply.started":"2024-08-09T22:37:01.867807Z","shell.execute_reply":"2024-08-09T22:37:01.872952Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"link_desc_df['split'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-08-09T22:37:01.875344Z","iopub.execute_input":"2024-08-09T22:37:01.875725Z","iopub.status.idle":"2024-08-09T22:37:01.896827Z","shell.execute_reply.started":"2024-08-09T22:37:01.875692Z","shell.execute_reply":"2024-08-09T22:37:01.895807Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"split\ntrain    175\ntest     123\ndev       52\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"link_desc_df.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T22:37:01.898335Z","iopub.execute_input":"2024-08-09T22:37:01.898700Z","iopub.status.idle":"2024-08-09T22:37:01.914317Z","shell.execute_reply.started":"2024-08-09T22:37:01.898670Z","shell.execute_reply":"2024-08-09T22:37:01.913357Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                          image_link  split  \\\n0  https://cdn.pixabay.com/photo/2020/10/03/11/08...  train   \n\n             probe_1                                      probe_2  \\\n0  There is handbag.  There dog looks eager to jump on the woman.   \n\n             probe_3                         probe_4  label_1  label_2  \\\n0  The dog is white.  The dog is far from the woman.    False    False   \n\n   label_3  label_4  \n0    False    False  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_link</th>\n      <th>split</th>\n      <th>probe_1</th>\n      <th>probe_2</th>\n      <th>probe_3</th>\n      <th>probe_4</th>\n      <th>label_1</th>\n      <th>label_2</th>\n      <th>label_3</th>\n      <th>label_4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://cdn.pixabay.com/photo/2020/10/03/11/08...</td>\n      <td>train</td>\n      <td>There is handbag.</td>\n      <td>There dog looks eager to jump on the woman.</td>\n      <td>The dog is white.</td>\n      <td>The dog is far from the woman.</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df = link_desc_df[link_desc_df['split'] == 'train']\ntest_df = link_desc_df[link_desc_df['split'] == 'test']\nval_df = link_desc_df[link_desc_df['split'] == 'dev']","metadata":{"execution":{"iopub.status.busy":"2024-08-09T22:37:01.915617Z","iopub.execute_input":"2024-08-09T22:37:01.915952Z","iopub.status.idle":"2024-08-09T22:37:01.925592Z","shell.execute_reply.started":"2024-08-09T22:37:01.915923Z","shell.execute_reply":"2024-08-09T22:37:01.924575Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-L/14@336px\", device=device)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T22:37:01.926709Z","iopub.execute_input":"2024-08-09T22:37:01.926976Z","iopub.status.idle":"2024-08-09T22:37:19.765838Z","shell.execute_reply.started":"2024-08-09T22:37:01.926953Z","shell.execute_reply":"2024-08-09T22:37:19.765032Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|████████████████████████████████████████| 891M/891M [00:05<00:00, 175MiB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, dataframe):\n        self.df = dataframe\n        self.cache = {}\n\n    def __len__(self):\n        return len(self.df) * 4\n\n    def __getitem__(self, idx):\n        img_url = self.df.iloc[idx//4, 0]\n        probe = self.df.iloc[idx//4, 2 + idx%4]\n        label = torch.tensor(self.df.iloc[idx//4, 6 + idx%4], dtype=torch.int)\n        text = clip.tokenize(probe).to(device)\n        with torch.no_grad():\n            probe_embedding = model.encode_text(text)\n        # Open image\n        if self.cache.get(idx) is not None:\n            image_embedding = self.cache[idx]\n        else:\n            image = preprocess(Image.open(requests.get(img_url, stream=True).raw)).unsqueeze(0).to(device)\n            with torch.no_grad():\n                image_embedding = model.encode_image(image)\n            self.cache[idx] = image_embedding\n        return image_embedding.squeeze().to(torch.float), probe_embedding.squeeze().to(torch.float), label.to(torch.float).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T22:37:53.497219Z","iopub.execute_input":"2024-08-09T22:37:53.497570Z","iopub.status.idle":"2024-08-09T22:37:53.508034Z","shell.execute_reply.started":"2024-08-09T22:37:53.497541Z","shell.execute_reply":"2024-08-09T22:37:53.506952Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 4\nEMBED_DIM = 768\n\ntrain_dataset = CustomDataset(train_df)\ntest_dataset = CustomDataset(test_df)\nval_dataset = CustomDataset(val_df)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T22:37:53.672417Z","iopub.execute_input":"2024-08-09T22:37:53.672781Z","iopub.status.idle":"2024-08-09T22:37:53.679445Z","shell.execute_reply.started":"2024-08-09T22:37:53.672751Z","shell.execute_reply":"2024-08-09T22:37:53.678439Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class EmbeddingClassifier(nn.Module):\n    def __init__(self):\n        super(EmbeddingClassifier, self).__init__()\n        # Define a fully connected layer to combine the two embeddings\n        self.fc1 = nn.Linear(EMBED_DIM * 2, 512)  # 768 * 2 because we are concatenating two embeddings\n        self.fc2 = nn.Linear(512, 128)\n        self.fc3 = nn.Linear(128, 1)  # Output layer\n        \n        # Activation functions\n        self.relu = nn.ReLU()\n        self.dropout1 = nn.Dropout(p=0.3)\n        self.dropout2 = nn.Dropout(p=0.5)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, embed1, embed2):\n        # Concatenate the two embeddings\n        x = self.dropout1(torch.cat((embed1, embed2), dim=1))\n        # Forward pass through the network\n        x = self.dropout2(self.relu(self.fc1(x)))\n        x = self.dropout2(self.relu(self.fc2(x)))\n        x = self.fc3(x)\n        \n        # Apply sigmoid to output a probability\n        x = self.sigmoid(x)\n        \n        return x\n    \n    def save_model(self, path):\n        \"\"\"Save the model state dictionary to the specified path.\"\"\"\n        torch.save(self.state_dict(), path)\n        print(f\"Model saved to {path}\")\n\n    def load_model(self, path):\n        \"\"\"Load the model state dictionary from the specified path.\"\"\"\n        self.load_state_dict(torch.load(path))\n        self.eval()  # Set the model to evaluation mode\n        print(f\"Model loaded from {path}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-09T22:43:42.915714Z","iopub.execute_input":"2024-08-09T22:43:42.916078Z","iopub.status.idle":"2024-08-09T22:43:42.926674Z","shell.execute_reply.started":"2024-08-09T22:43:42.916048Z","shell.execute_reply":"2024-08-09T22:43:42.925736Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"num_epochs = 50\nLR = 1e-4\n\nmy_model = EmbeddingClassifier().to(device)\nprint(sum(p.numel() for p in my_model.parameters() if p.requires_grad))\noptimizer = torch.optim.Adam(my_model.parameters(), lr=LR)\n\n# Early stopping parameters\npatience = 5  # Number of epochs to wait before stopping if no improvement\nbest_val_loss = float('inf')  # Initialize to infinity\ncounter = 0  # Counter for early stopping","metadata":{"execution":{"iopub.status.busy":"2024-08-09T22:43:43.122237Z","iopub.execute_input":"2024-08-09T22:43:43.122586Z","iopub.status.idle":"2024-08-09T22:43:43.139718Z","shell.execute_reply.started":"2024-08-09T22:43:43.122557Z","shell.execute_reply":"2024-08-09T22:43:43.138795Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"852737\n","output_type":"stream"}]},{"cell_type":"code","source":"# def f1_loss(y_pred, y_true):\n#     tp = torch.sum((y_true * y_pred).float(), dim=0)\n#     tn = torch.sum(((1 - y_true) * (1 - y_pred)).float(), dim=0)\n#     fp = torch.sum(((1 - y_true) * y_pred).float(), dim=0)\n#     fn = torch.sum((y_true * (1 - y_pred)).float(), dim=0)\n\n#     p = tp / (tp + fp + 1e-7)\n#     r = tp / (tp + fn + 1e-7)\n\n#     f1 = 2 * p * r / (p + r + 1e-7)\n#     f1 = torch.where(torch.isnan(f1), torch.zeros_like(f1), f1)\n#     return 1 - torch.mean(f1)\n\ncriterion = nn.BCELoss()","metadata":{"execution":{"iopub.status.busy":"2024-08-09T22:43:43.304429Z","iopub.execute_input":"2024-08-09T22:43:43.305078Z","iopub.status.idle":"2024-08-09T22:43:43.309613Z","shell.execute_reply.started":"2024-08-09T22:43:43.305046Z","shell.execute_reply":"2024-08-09T22:43:43.308691Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Training loop\nfor epoch in range(num_epochs):\n    my_model.train()  # Set model to training mode\n    train_loss = 0.0\n\n    for img_embed, desc_embed, label in tqdm(train_loader):\n        optimizer.zero_grad()  # Zero the parameter gradients\n        # Forward pass\n        output = my_model(img_embed, desc_embed)\n        loss = criterion(output, label.unsqueeze(1))\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * img_embed.size(0)  # Accumulate train loss\n\n    # Calculate average train loss\n    avg_train_loss = train_loss / len(train_loader.dataset)\n    \n    # Validation step\n    my_model.eval()  # Set model to evaluation mode\n    val_loss = 0.0\n\n    with torch.no_grad():\n        for img_embed, desc_embed, label in tqdm(val_loader):\n            output = my_model(img_embed, desc_embed)\n            loss = criterion(output, label.unsqueeze(1))\n            val_loss += loss.item() * img_embed.size(0)  # Accumulate validation loss\n\n    # Calculate average validation loss\n    avg_val_loss = val_loss / len(val_loader.dataset)\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n\n    # Check if the validation loss improved\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        counter = 0\n        my_model.save_model('best_model.pth')\n        print(f\"Validation loss improved, saving model to best_model.pth\")\n    else:\n        counter += 1\n        print(f\"Validation loss did not improve for {counter} epochs\")\n\n    # Early stopping\n    if counter >= patience:\n        print(\"Early stopping triggered\")\n        break","metadata":{"execution":{"iopub.status.busy":"2024-08-09T22:43:43.538487Z","iopub.execute_input":"2024-08-09T22:43:43.538869Z","iopub.status.idle":"2024-08-09T22:45:04.741451Z","shell.execute_reply.started":"2024-08-09T22:43:43.538839Z","shell.execute_reply":"2024-08-09T22:45:04.740577Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"  0%|          | 0/175 [00:00<?, ?it/s]/tmp/ipykernel_34/543969151.py:12: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n  label = torch.tensor(self.df.iloc[idx//4, 6 + idx%4], dtype=torch.int)\n100%|██████████| 175/175 [00:06<00:00, 27.95it/s]\n100%|██████████| 52/52 [00:01<00:00, 27.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/50], Train Loss: 0.6935, Val Loss: 0.6877\nModel saved to best_model.pth\nValidation loss improved, saving model to best_model.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 175/175 [00:06<00:00, 27.81it/s]\n100%|██████████| 52/52 [00:01<00:00, 28.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/50], Train Loss: 0.6887, Val Loss: 0.6864\nModel saved to best_model.pth\nValidation loss improved, saving model to best_model.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 175/175 [00:06<00:00, 27.92it/s]\n100%|██████████| 52/52 [00:01<00:00, 29.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/50], Train Loss: 0.6787, Val Loss: 0.6839\nModel saved to best_model.pth\nValidation loss improved, saving model to best_model.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 175/175 [00:06<00:00, 27.14it/s]\n100%|██████████| 52/52 [00:01<00:00, 28.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/50], Train Loss: 0.6577, Val Loss: 0.6845\nValidation loss did not improve for 1 epochs\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 175/175 [00:06<00:00, 27.88it/s]\n100%|██████████| 52/52 [00:01<00:00, 28.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/50], Train Loss: 0.6362, Val Loss: 0.6799\nModel saved to best_model.pth\nValidation loss improved, saving model to best_model.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 175/175 [00:06<00:00, 27.84it/s]\n100%|██████████| 52/52 [00:01<00:00, 28.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/50], Train Loss: 0.6236, Val Loss: 0.6953\nValidation loss did not improve for 1 epochs\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 175/175 [00:06<00:00, 28.07it/s]\n100%|██████████| 52/52 [00:01<00:00, 28.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/50], Train Loss: 0.5954, Val Loss: 0.6900\nValidation loss did not improve for 2 epochs\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 175/175 [00:06<00:00, 27.61it/s]\n100%|██████████| 52/52 [00:01<00:00, 29.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/50], Train Loss: 0.5692, Val Loss: 0.7010\nValidation loss did not improve for 3 epochs\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 175/175 [00:06<00:00, 27.59it/s]\n100%|██████████| 52/52 [00:01<00:00, 28.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/50], Train Loss: 0.5632, Val Loss: 0.7087\nValidation loss did not improve for 4 epochs\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 175/175 [00:06<00:00, 27.83it/s]\n100%|██████████| 52/52 [00:01<00:00, 29.09it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch [10/50], Train Loss: 0.5316, Val Loss: 0.7222\nValidation loss did not improve for 5 epochs\nEarly stopping triggered\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}